\section{Factoring}
\label{sec:factor}

Shor's factoring algorithm in 1994 was the first and most famous example of an
exponential speedup of quantum computers over classical computers
\cite{Shor1994}. However, like the QFT, it is beginning to show its age.
The real problem now becomes remaining excited about prime factorization,
which Gauss exhorted us to solve those long centuries ago.

Although we study this problem in greater depth in a separate work
\cite{Pham2012b}, we mention here some interesting techniques introduced
specifically for factoring but which may have wider applications.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Uninitialized Qubits and\\ Approximate Algorithms}

Parker and Plenio \cite{Parker2000} discovered that Shor's factoring algorithm
could still have a low error probability even if $n$ of the $2n$ ancillae qubits
were unitialized. That is, they introduced two important notions: approximate
algorithms and unitialized qubits.
An \emph{approximate algorithm} is one which succeeds for a certain fraction of
its ancillary inputs, if all initial values are possible with uniform probability.
The success probability is determined by the structure of the algorithm.
For the example of factoring, the algorithm will fail if the ancillae
register for modular multiplication starts out in a state $\ket{\alpha}$
where $\alpha$ is a number that is not coprime with the modulus to be factored,
$m$. If $m$ is truly prime, then all $\alpha$ will be corprime to it, but
if $m$ is not a \emph{smooth number} (that is, if $m$ is a product of many prime
factors) then a significant fraction of initial values $\ket{\alpha}$ will
cause the algorithm to fail.

Zalka further improved this result so that none of
the ancillae need be initialized \cite{Zalka2006}, although at the cost of
a more complicated modular multiplication algorithm.
To overcome adversarial inputs,
he rrandomizes the ancillae
%by randomly applying Pauli operators to the ancillae%
before beginning the algorithm.
However, both the Zalka and Parker-Plenio result assume the ancillae qubits
can be in a superposition of computational basis states, but are otherwise
pure states in that they are not entangled with the environment or any other
qubits. This may not be a realistic assumption in practice for uncooled
qubits in the laboratory. A more realistic model, for the power of a
single (potentially mixed) qubit is given by Knill and LaFlamme in their
definition of DQC1, the complexity class of problems computable with one
uninitialized qubit \cite{Knill1998}.

Nevertheless, unitialized qubits are an alternative circuit resource which
may be considered in the future and share a lot of features in common with
magic state distillation, which we discuss in Section \ref{sec:magic}. In
contrast to circuit size, width, and depth, it is advantageous for
an architecture or algorithm
to allow more uninitialized qubits rather than fewer.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Approximate Modular\\ Multiplication}

Zalka also studied an earlier idea of approximation in factoring, namely
approximate modular multiplication \cite{Zalka1998}.
This idea is instrumental in Kutin's linear-depth modular multiplier,
used to achieve a 1D NTC architecture for factoring in $O(n^2)$ depth
\cite{Kutin2006}. In factoring, we perform several rounds of period-finding,
choosing a random $a \in \mathbb{Z}_m$ for factoring an $n$-bit modulus $m$.
We must then
perform $2n$ multiplications of $a$ raised to various powers of two
into an $n$-qubit quantum register containing $\ket{b}$, where
$b = \sum_{i} 2^ib_i$. Each
modular multiplication must compute the $n$-bit modular product
$r$ as in Equation \ref{eqn:modmult-r}.

\begin{eqnarray}
r & \equiv & abc \\
  & \equiv & \sum_{i} 2^i a b_i c_j \\
  & \equiv & \sum_{i} (b_i c_j)(2^i a^{2^j} \bmod m) \bmod m
\label{eqn:modmult-r}
\end{eqnarray}

This involves adding $2n$ classically precomputed numbers
$(2^i a^{2^j} \bmod m)$ which are controlled on a bit $c_j$
which is later measured as
part of phase estimation. Normal, non-modular multiplication can be done
in linear-depth using Draper's transform adder \cite{Draper2000}. This gives
us the sum $s$ in Equation \ref{eqn:modmult-s}. Here we rewrite the
bit $b_i c$ as $y_i$ and the $n$-bit number $2^i a^{2^j} \bmod m$ as
$x_i$. However,
the modular reduction part is the most resource-intensive, that is,
determining $q$ and them subtracting $qm$ from $s$ to get $r$.

\begin{eqnarray}
s & = & \sum_i b_i c_j (2^i a^{2^j} \bmod m) \\
  & = & \sum_i y_i x_i = r + qm
\label{eqn:modmult-s}
\end{eqnarray}

Note that $q$ can be encoded in $\lceil \log_2 n \rceil$ bits, and we can
express it as a sum followed by a division as in Equation \ref{eqn:modmult-q}.

\begin{equation}
q = \lfloor ( \sum_{i} y_i x_i ) / m \rfloor
\label{eqn:modmult-q}
\end{equation}

In general, if we wanted to obtain $q$ by the division $\lfloor s/m \rfloor$,
we would perform $\lceil \log_2 n \rceil$ trial subtractions of the
$n$-bit numbers $2^{k-1} m$ to determine the
bits $q_k$, for $k \in \{1, \ldots, n\}$. However, in the transform adder,
we would have to change into and out of the Fourier basis every time to
compare the value of the high bit $q_k$ to see if we underflowed the register
or not. This would normally take depth $O(n^2)$.

Zalka's key idea, developed by Kutin, is that we can approximate $q$ by
$\hat{q}$, where in the sum in Equation \ref{eqn:modmult-q} we replace
$x_i$ with $\hat{x}_i$, where we keep the upper $\ell$ bits and set the
lower $n - \ell$ bits to 0. Now we are only doing trial subtractions with
$\log_2 n$-bit numbers, and we can compute $\hat{q}$ in depth $O(\log^2 n)$.

What is the error in this approximation? With probability $2^{-\ell}$,
$\hat{q} \ne q$. If we set $\ell = 2\log_2 n$, we get a constant error
probability. The intuitive idea of this is that for some values of
$a \in \mathbb{Z}_m$, the lower $n - \ell$ bits of $x_i$ add up to more than
$m$ and cause $\hat{q}$ to be greater than $q$. We know which values of $a$
will cause a problem in advance, but we must also choose $a$ randomly for
quantum period-finding to succeed in factoring.

Unfortunately, this technique is unlikely to improve the 2D NTC factoring implement of
Ref. \cite{Pham2012b} which already has constant-depth modular reduction.
However, it might serve as inspiration for future approximate algorithms.

%What I want to emphasize from this algorithm is the approximation of
%$q$ by neglecting the low-order bits of $x$. As long as the sum of these
%low-order bits do not equal $m$, then $\hat{q}$ and $q$ are approximately
%equal, and no error occurs. Here $\hat{q}$ is calculated using the
%top $\ell_0$ bits of each $x_i$, where
%$\hat{x}_i = 2^{n-\ell_0}\lfloor x_i \rfloor$, that is $x_i$ with the
%bottom $n-\ell_0$ bits set to 0.

%\begin{equation}
%\hat{q} = \lfloor (\sum_{i} y_i \hat{x}_i ) / m \rfloor
%\end{equation}

%The steps to compute $q$ are to perform division via the following algorithm
%For bits $k = \log_2 n$ downto 1, subtract $2^{k-1}m$ from the sum $s$ as a
%trial. Apply an inverse QFT to get the high-order bit
%$q_k$ in the computational
%basis, which is $1$ if there is an overflow (in 2's complement representation).
%Convert back into the Fourier basis with another QFT, and use $q_k$ to
%control addition of $2^{k-1}m$ back if necessary.
%Since we are only operating on $O(\log_2 n)$ bits, this somewhat inefficient
%process nevertheless only takes depth $O(\log^2 n)$.

%We can calculate the circuit space of this block as follows.

%The success probability is determined by the parameter $\ell_0$. To get
%a constant, we set $\ell_0 = 2\log_2 n$ something something, oh god please
%fact check this.

%Then the space-success product is

%\begin{equation}
%\frac{\log^3 n}{\epsilon}
%\end{equation}