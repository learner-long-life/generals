\section{Factoring}
\label{sec:factor}

Shor's factoring algorithm in 1994 was the first and most famous example of an
exponential speedup of quantum computers over classical computers.
\cite{Shor1994}
As such, it is still an active area of research, both in optimizing the
circuit \cite{Cleve2002} and the experimental implementation. (actually,
I don't know of any
recent experiments that are getting close to doing this again).

Although we study this problem in greater depth in a separate work
\cite{Pham2012b}, we mention here some interesting techniques introduced
specifically for factoring but which may have wider applications.

\subsection{Modular Entangled States}

In most quantum computations, qubits become part of a larger and larger
entangled state throughout the computation until finally the output
qubits are measured, and the entire system collapses to one of the
computational basis states. This is the reason why we uncompute ancillae,
to reduce the size of the entangled state, and in some sense this is what
we are measuring with the circuit space resource.

However, such a large entangled state is vulnerable to decoherence, where
a single error anywhere can destroy the entire state. In an encoded state,
it may take more errors, but in general we might like to keep the
state as modular as possible, the product of multiple smaller entangled
states. If we have lots of parallelism available, we may choose to
prepare multiple redundant copies of an entangled state.
If unrecoverable errors are detected in any block, we can discard
that block and use a equivalent one from the same set.
In this way, we probabilistically build up a larger entangled state out of
a product state, similar to the post-selection approach of Knill
(citation neeed here).

However, in many computations, especially those that we can parallelize,
like the phase estimation procedure of KSV or Cleve-Watrous,
many states are prepared in parallel and only entangled at the end.
Therefore, we propose an approach of modular entanglement.

\subsection{Zalka's Uninitialized Qubits}

In the circuit resources we mentioned in Section \ref{sec:circuit},
we introduced uninitialized qubits. Well, here they are.

Zalka's definition of uninitialized qubits is that they start in an
arbitrary state $\ket{\alpha}$ instead of $\ket{0}$. He defines
\emph{approximative} algorithms as those which succeed for ``most'' inputs,
say for $1/n$ fraction of them. Since some states may be bad and we may be
worried about adversarial noise feeding us these bad states as input, we can
randomly initialize the states into a maximally-mixed state by applying
random Pauli operators to every qubit. In reality, we may do this by
using classical random bits to control these Pauli operators, and then discarding
these classical bits.

I'm not convinced this actually works to create the maximally mixed state
$\rho = I / d$, or that this is the same as an equal superposition of computational
basis states, which Zalka states is what he considers a "mixed" state at one
point. For example, if we start in the cat state, in some sense the
``maximally entangled'' state, applying random unitaries just creates a
cat state up to these Pauli corrections. He also assumes if you have two
registers, say $\ket{\alpha}$ and $\ket{\beta}$, that they are not entangled,
or at least that you can randomize them so that their entanglement does not
hurt you.

Furthermore, the extra work we have to do to randomize the inputs may
negate the benefits of not having to cool down these qubits. I don't know
the actual experimental physics involved, but it seems that most of the work
is in cooling down, say, trapped ions, to the lowest two energy states.
After that, cooling down to the ground state is relatively easy. So it's not
clear that we are saving that much work.

\subsection{Zalka's Coset Representation}

In the same work \cite{Zalka2002} Zalka also introduces the idea of a
coset representation, an alternative basis for encoding integers that
makes approximate modular addition easy. In this representation,
an integer $\ket{b}$ is transformed into a superposition of itself
plus multiples of the modulus $N$.

\begin{equation}
\ket{b} \rightarrow \sum_{x} \ket{b + xN}
\end{equation}

The spectrum picture of this transformation is shown in
Figure \ref{fig:coset-spectrum}

\begin{figure}
\label{fig:coset-spectrum}
\caption{The spectrum picture of the coset representation}
\end{figure}

In this representation, adding another classical number $a$ modulo $N$
takes the following form:

\begin{equation}
\ket{b+a} \rightarrow \ket{y} = \sum_{x} \ket{b + a + xN} \approx
\ket{\hat{y}} = \sum_{x} \ket{b + a + xN \bmod N}
\end{equation}

The second approximately equals sign is due to the fact that given
sufficiently large $x_max$, the overlap between $\ket{y}$ and
$\ket{\hat{y}}$ is still large. In a sense, we just neglect modular reduction
and allow the periodic structure of our coset representation to ``absorb''
these errors. We must therefore increase the size of the register holding
$\ket{y}$. For $4n^2$ modular additions, such as those required in Shor's
algorithm, it suffices to set $x_max = 10000n^2$ (FACT CHECK THIS) and
therefore the size of the register to be $n + 2\log{n}$ bits (ALSO FACT
CHECK THIS).

The error / overlap is calculated as follows:

\begin{equation}
\bra{\hat{y}}\ket{y} = 1 - \frac{1}{x_{max}}
\end{equation}

Of course, converting between the coset representation and the computational
basis is rather inefficient. This technique is also used in
Hales and Hallgren (cite appropriate work here).

\subsection{Kutin's 1D NTC Factoring}

Zalka's ideas appear again in Kutin's 1D NTC architecture for factoring
\cite{Kutin2006}
where they are used to get a remarkable linear-depth modular multiplier.
While they build on the 1D NTC work of Fowler, Devitt, and Hollenberg
\cite{Fowler2004} using the triangular QFT, they also introduce the
idea of approximating the modular multiple $q$ in the following
equation.

\begin{equation}
s = \sum_i b_i c (2^i a \bmod m) = r + qm
\end{equation}

What I want to emphasize from this algorithm is the approximation of
$q$ by neglecting the low-order bits of $x$. As long as the sum of these
low-order bits do not equal $m$, then $\hat{q}$ and $q$ are approximately
equal, and no error occurs. Here $\hat{q}$ is calculated using the
top $\ell_0$ bits of each $x_i$, where
$\hat{x}_i = 2^{n-\ell_0}\lfloor x_i \rfloor$, that is $x_i$ with the
bottom $n-\ell_0$ bits set to 0.

\begin{equation}
\hat{q} = \lfloor (\sum_{i} y_i \hat{x}_i ) / m \rfloor
\end{equation}

The steps to compute $q$ are to perform division via the following algorithm
For bits $k = \log_2 n$ downto 1, subtract $2^{k-1}m$ from the sum $s$ as a
trial. Apply an inverse QFT to get the high-order bit
$q_k$ in the computational
basis, which is $1$ if there is an overflow (in 2's complement representation).
Convert back into the Fourier basis with another QFT, and use $q_k$ to
control addition of $2^{k-1}m$ back if necessary.
Since we are only operating on $O(\log_2 n)$ bits, this somewhat inefficient
process nevertheless only takes depth $O(\log^2 n)$.

We can calculate the circuit space of this block as follows.

The success probability is determined by the parameter $\ell_0$. To get
a constant, we set $\ell_0 = 2\log_2 n$ something something, oh god please
fact check this.

Then the space-success product is

\begin{equation}
\frac{\log^3 n}{\epsilon}
\end{equation}